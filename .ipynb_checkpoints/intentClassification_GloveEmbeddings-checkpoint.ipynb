{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# we will be using pre trained glove word vectors which has 400k words in vocab from wikipedia\n",
    "# we have to run demo.sh to obtain the vocab.txt, which obtains 100M characters of Wikipedia\n",
    "\n",
    "# here we will make a class of embedding and make some attributes like vocab and all \n",
    "# which can be used later\n",
    "#self.vocab\n",
    "#self.W\n",
    "\n",
    "#the file vocab.txt is having the list of words along with some number(I dont know what that is now)\n",
    "# the file vectors.txt is having word and its vector ahead in a line.\n",
    "\n",
    "class gloveEmbeddings:\n",
    "    \n",
    "    def __init__(self, vocabFile, vectorFile):\n",
    "        \n",
    "        f = open(vocabPath, 'r')\n",
    "        #we take out all words and put in words list\n",
    "        words = [x.rstrip().split(' ')[0] for x in f.readlines()]\n",
    "\n",
    "        # now we ll make a dictionary with key as the word and value as 50 D vector\n",
    "        # from vector.txt\n",
    "\n",
    "        f = open(vectorPath, 'r')\n",
    "        vectors = {}\n",
    "\n",
    "        for l in f:\n",
    "            values = l.rstrip().split(' ')\n",
    "            vectors[values[0]] = [float(num) for num in values[1:]]\n",
    "        \n",
    "        #now our aim is to make a big numpy matrix of (vocabSize, vectorSize)\n",
    "        # which will be holding vector values for each word with regard to the position number of \n",
    "        # the word\n",
    "        vocabSize = words.__len__()\n",
    "        \n",
    "        #now we have to take record of position of words\n",
    "        \n",
    "        # ennumerate in python is used for loop over something with an automatic counter\n",
    "        # for counter, word in enumerate(list):\n",
    "        #here counter will be keeping the count\n",
    "        \n",
    "        \n",
    "        vocab = {word : position for position, word in enumerate(words)}\n",
    "        posVocab = {position : word for position, word in enumerate(words)}\n",
    "        \n",
    "        vectorSize = 50\n",
    "        \n",
    "        W = np.zeros((vocabSize, vectorSize))\n",
    "        \n",
    "        for word, vec in vectors.items():   #dictionary.items() give tuple of key, value pair\n",
    "            \n",
    "            if word == '<unk>':\n",
    "                continue\n",
    "                \n",
    "            W[vocab[word], :] = vec   #here vocab[word] gives the position of that word\n",
    "            \n",
    "        # now we will normalize values of W\n",
    "\n",
    "        wNew = np.zeros(W.shape)\n",
    "\n",
    "        d = (np.sum(W ** 2, 1) ** (0.5))    #in sum(W**2, 1), 1 is used for axis. 1 means summing across the \n",
    "                                            #columns, ie. producing total in one row\n",
    "        wNew = (W.T / d).T\n",
    "\n",
    "        # things are done, now we will set attributes so that we can access through this attribute\n",
    "\n",
    "        self.vocab = vocab\n",
    "        self.W = wNew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sumVectors(embed, query):\n",
    "    \n",
    "    tokens = query.split(' ')\n",
    "    \n",
    "    # now we will initialize a np array of size equal to dimension of vec and then keep \n",
    "    #on adding the equivalent vec of words of query\n",
    "    \n",
    "    vec = np.zeros(embed.W.shape[1])\n",
    "    \n",
    "    for word in tokens:\n",
    "        \n",
    "        if word in embed.vocab:\n",
    "            \n",
    "            vec = vec + embed.W[embed.vocab[word], :]  #again vocab[word] gives position of that word\n",
    "            \n",
    "    return vec\n",
    "\n",
    "def calculateCentroid(embed, samples):\n",
    "    \n",
    "    vec = np.zeros((len(samples), embed.W.shape[1]))\n",
    "    \n",
    "    for count, sample in enumerate(samples):\n",
    "        \n",
    "        vec[count, :] = sumVectors(embed, sample)\n",
    "        \n",
    "    centroid = np.mean(vec, axis = 0)\n",
    "    \n",
    "    assert centroid.shape[0] == embed.W.shape[1]\n",
    "    \n",
    "    return centroid\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getDataFromCsv(csvPath):\n",
    "    \n",
    "    df = pd.read_csv(csvPath)\n",
    "    df = df.fillna(0)\n",
    "    \n",
    "    classNum = len(df.columns)\n",
    "    classNames = df.columns.values.tolist()\n",
    "    \n",
    "    dicti = {}\n",
    "    for colName in classNames:\n",
    "        lis = []\n",
    "        for sentence in df[colName]:\n",
    "            if sentence != 0:\n",
    "                lis.append(str(sentence).lower().rstrip())\n",
    "\n",
    "            dicti[colName] = { \"samples\" : lis, \"centroid\" : None }\n",
    "            \n",
    "    for label in classNames:\n",
    "        dicti[label][\"centroid\"] = calculateCentroid(embed,dicti[label][\"samples\"])\n",
    "    \n",
    "    return dicti,classNum,classNames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def intentClassifier(embed, query):\n",
    "    \n",
    "    dicti, classNum, classNames = getDataFromCsv(os.path.abspath('bot_intent_for_model.csv'))\n",
    "    \n",
    "    vec = sumVectors(embed, query)\n",
    "    \n",
    "    scores = np.array([ np.linalg.norm(vec-dicti[label][\"centroid\"]) for label in classNames ])\n",
    "    \n",
    "    return classNames[np.argmin(scores)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabPath = os.path.abspath('glove_vectors_wikipedia/vocab.txt')\n",
    "vectorPath = os.path.abspath('glove_vectors_wikipedia/vectors.txt')\n",
    "\n",
    "flag = 'y'\n",
    "while flag == 'y':\n",
    "    \n",
    "    embed = gloveEmbeddings(vocabPath, vectorPath)\n",
    "    \n",
    "    query = str(input(\"Enter your query : \"))\n",
    "    \n",
    "    intent = intentClassifier(embed,query)\n",
    "    \n",
    "    print('\\n classified intent : {}'.format(intent))\n",
    "    \n",
    "    print('\\n you want to continue? (y/n)')\n",
    "    \n",
    "    flag = input()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
